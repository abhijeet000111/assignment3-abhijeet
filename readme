Vanilla Seq2Seq and Attention-based Seq2Seq Model

This repository contains the implementation of vanilla Sequence-to-Sequence (Seq2Seq) and attention-based Seq2Seq models using PyTorch. The project utilizes the Aksharantar dataset for training, validating, and testing the models. Additionally, it includes hyperparameter tuning using Weights & Biases (W&B).

Table of Contents
1. Installation
2. Dataset
3. Model Architecture
4. Training
5. Evaluation
6. Hyperparameter Tuning
7. Usage
8. Acknowledgements
9. License

Installation
To get started, clone this repository and install the required dependencies:

git clone https://github.com/abhijeet000111/cs6910-assignment3-abhijeet.git
cd vanilla-attention-seq2seq
pip install -r requirements.txt

Dataset
The dataset used in this project is the Aksharantar dataset, which contains parallel corpus data. The dataset can be downloaded using gdown and unzipped as follows:

import gdown

# Google Drive file URL
url = 'https://drive.google.com/uc?export=download&id=16yBIEtMjkb-8pmiHOk84YaLXpHrX25cW'
output_file = 'my_aksharantar_dataset_sampled.zip'
# Download the file
gdown.download(url, output_file, quiet=False)

!unzip my_aksharantar_dataset_sampled.zip

The dataset is split into training, validation, and test sets located in the aksharantar_sampled/hin directory.

Model Architecture

Encoder
The encoder is a recurrent neural network (RNN) that processes the input sequence and outputs a context vector. The implementation supports GRU, LSTM, and vanilla RNN architectures.

Decoder
The decoder is also an RNN that generates the output sequence from the context vector produced by the encoder. Similar to the encoder, the decoder supports GRU, LSTM, and vanilla RNN architectures.

Attention Mechanism
The attention mechanism is implemented using Bahdanau attention. It helps the decoder focus on different parts of the input sequence at each step, improving translation accuracy.

Training
The training process includes the following steps:
1. Data Preparation: The dataset is preprocessed and converted into tensors.
2. Model Initialization: The encoder and decoder models are initialized.
3. Training Loop: The models are trained over several epochs with the specified hyperparameters.
4. Validation: The models are validated on the validation set after each epoch.

Example Training Command

python train.py --train_path aksharantar_sampled/hin/hin_train.csv --val_path aksharantar_sampled/hin/hin_valid.csv --test_path aksharantar_sampled/hin/hin_test.csv

Evaluation
The evaluation process involves generating translations for random sentences from the dataset and comparing them with the ground truth. The evaluation function prints the input, target, and output sequences for subjective quality assessment.

Example Evaluation Command

python evaluate.py --model_path models/seq2seq_model.pth

Hyperparameter Tuning
Hyperparameter tuning is performed using Weights & Biases (W&B). The configuration space for hyperparameters is defined, and a sweep is created to find the best hyperparameters.

Example Hyperparameter Tuning Command

wandb sweep sweep_config.yaml
wandb agent yourusername/projectname/sweepid

Usage

Training with W&B
To train the model with W&B, run the following command:

python train_wandb.py

Evaluating the Model
To evaluate the model, use the following command:

python evaluate.py --model_path path_to_your_trained_model.pth

Acknowledgements
This project is based on the Aksharantar dataset and uses PyTorch for model implementation. Special thanks to the creators of the dataset and the PyTorch community for their valuable resources and support.

Feel free to contribute to this project by creating issues or submitting pull requests. Your feedback and contributions are highly appreciated!

License
This project is licensed under the MIT License. See the LICENSE file for more details.
